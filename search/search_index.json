{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation This documentation describes the principal operations and configurations that can be made with the MLOps module. Getting Started : Basic operations of the MLOps module. Configuration : Configuration of basic parameters. API Documentation : Description of the API requests. Reference : Detailed documentation of each script function.","title":"Home"},{"location":"#documentation","text":"This documentation describes the principal operations and configurations that can be made with the MLOps module. Getting Started : Basic operations of the MLOps module. Configuration : Configuration of basic parameters. API Documentation : Description of the API requests. Reference : Detailed documentation of each script function.","title":"Documentation"},{"location":"getting_started/","text":"Getting Started Install MLOps module ~$ python -m venv venv ~$ source venv/bin/activate ~$ python -m pip install --upgrade pip setuptools wheel ~$ python -m pip install -e .[dev] Download dataset from external source # Connect to external source and pull files ~$ dvc pull # Should download the following files data/api_dataset.pkl data/dataset_credit_risk.csv models/api.joblib models/api_metrics.json Run the command line interface # For help, add the --help flag on any command ~$ mlops --help # Feature engineering ~$ mlops feature-engineering Clean dataset file created at data/api_dataset.pkl # Train model ~$ mlops train-model Model saved at models/api.joblib Metrics saved at models/api_metrics.json # Predict outcome ~$ mlops predict 28 4 34 128 0 Prediction: 1 Load the API # Start uvicorn server ~$ uvicorn app.api:app --port 5000 # Health check ~$ curl -X 'GET' 'http://localhost:5000/' -H 'accept: application/json' { \"message\": \"OK\", \"status_code\": 200, \"data\": {} } # Serve features request ~$ curl -X 'GET' 'http://localhost:5000/5008832' -H 'accept: application/json' { \"message\": \"OK\", \"status_code\": 200, \"data\": { \"user_id\": 5008832, \"found\": true, \"features\": { \"age\": 28, \"years_on_the_job\": 4, \"nb_previous_loans\": 34, \"avg_amount_loans_previous\": 128.7918863477761, \"flag_own_car\": 0 } } } # Prediction request ~$ curl -X 'GET' 'http://localhost:5000/5008832/predict' -H 'accept: application/json' { \"message\": \"OK\", \"status_code\": 200, \"data\": { \"user_id\": 5008832, \"found\": true, \"features\": { \"age\": 28, \"years_on_the_job\": 4, \"nb_previous_loans\": 34, \"avg_amount_loans_previous\": 128.7918863477761, \"flag_own_car\": 0 }, \"prediction\": 1 } } Testing # Unit test ~$ pytest -m unit # Model test ~$ pytest -m model # API test ~$ pytest -m api # Data test # Change tests/great_expactations/great_expectations.yml # base_directory parameter to the absulute data directory ~$ cd tests ~$ great_expectations checkpoint run credit_risk ~$ great_expectations checkpoint run api ~$ cd .. Docker container # Locally ~$ docker build -t mlops:latest . ~$ docker run -d -p 5000:80 --name mlops mlops:latest Python style (PEP8) # Adhere to PEP8 python style guide ~$ black . # Show mismatching rules ~$ flake8 # Sort imports ~$ isort . GitHub CI/CD workflow Triggers on each push to the main branch. Requires manual approval to deploy to production environment. The GutHub project is deployed on Amazon EC2 using GitHub Actions and AWS CodeDeploy. Call API from production environment ~$ curl -X 'GET' 'http://{ec2-user-ip}:80/' -H 'accept: application/json'","title":"Gettin Started"},{"location":"getting_started/#getting-started","text":"","title":"Getting Started"},{"location":"getting_started/#install-mlops-module","text":"~$ python -m venv venv ~$ source venv/bin/activate ~$ python -m pip install --upgrade pip setuptools wheel ~$ python -m pip install -e .[dev]","title":"Install MLOps module"},{"location":"getting_started/#download-dataset-from-external-source","text":"# Connect to external source and pull files ~$ dvc pull # Should download the following files data/api_dataset.pkl data/dataset_credit_risk.csv models/api.joblib models/api_metrics.json","title":"Download dataset from external source"},{"location":"getting_started/#run-the-command-line-interface","text":"# For help, add the --help flag on any command ~$ mlops --help # Feature engineering ~$ mlops feature-engineering Clean dataset file created at data/api_dataset.pkl # Train model ~$ mlops train-model Model saved at models/api.joblib Metrics saved at models/api_metrics.json # Predict outcome ~$ mlops predict 28 4 34 128 0 Prediction: 1","title":"Run the command line interface"},{"location":"getting_started/#load-the-api","text":"# Start uvicorn server ~$ uvicorn app.api:app --port 5000 # Health check ~$ curl -X 'GET' 'http://localhost:5000/' -H 'accept: application/json' { \"message\": \"OK\", \"status_code\": 200, \"data\": {} } # Serve features request ~$ curl -X 'GET' 'http://localhost:5000/5008832' -H 'accept: application/json' { \"message\": \"OK\", \"status_code\": 200, \"data\": { \"user_id\": 5008832, \"found\": true, \"features\": { \"age\": 28, \"years_on_the_job\": 4, \"nb_previous_loans\": 34, \"avg_amount_loans_previous\": 128.7918863477761, \"flag_own_car\": 0 } } } # Prediction request ~$ curl -X 'GET' 'http://localhost:5000/5008832/predict' -H 'accept: application/json' { \"message\": \"OK\", \"status_code\": 200, \"data\": { \"user_id\": 5008832, \"found\": true, \"features\": { \"age\": 28, \"years_on_the_job\": 4, \"nb_previous_loans\": 34, \"avg_amount_loans_previous\": 128.7918863477761, \"flag_own_car\": 0 }, \"prediction\": 1 } }","title":"Load the API"},{"location":"getting_started/#testing","text":"# Unit test ~$ pytest -m unit # Model test ~$ pytest -m model # API test ~$ pytest -m api # Data test # Change tests/great_expactations/great_expectations.yml # base_directory parameter to the absulute data directory ~$ cd tests ~$ great_expectations checkpoint run credit_risk ~$ great_expectations checkpoint run api ~$ cd ..","title":"Testing"},{"location":"getting_started/#docker-container","text":"# Locally ~$ docker build -t mlops:latest . ~$ docker run -d -p 5000:80 --name mlops mlops:latest","title":"Docker container"},{"location":"getting_started/#python-style-pep8","text":"# Adhere to PEP8 python style guide ~$ black . # Show mismatching rules ~$ flake8 # Sort imports ~$ isort .","title":"Python style (PEP8)"},{"location":"getting_started/#github-cicd-workflow","text":"Triggers on each push to the main branch. Requires manual approval to deploy to production environment. The GutHub project is deployed on Amazon EC2 using GitHub Actions and AWS CodeDeploy.","title":"GitHub CI/CD workflow"},{"location":"getting_started/#call-api-from-production-environment","text":"~$ curl -X 'GET' 'http://{ec2-user-ip}:80/' -H 'accept: application/json'","title":"Call API from production environment"},{"location":"app/api/","text":"API Documentation The MLOps API can be requested from the localhost on port 5000 . predict ( user_id ) Predict status from user ID features. Retrieve most recent features and make a prediction on the status . cURL command: curl -X 'GET' 'http://localhost:5000/{user_id}/predict' -H 'accept: application/json' Parameters: user_id ( int ) \u2013 User ID. Returns: dict \u2013 Response with prediction . Source code in app/api.py @app . get ( \"/ {user_id} /predict\" , tags = [ \"Prediction\" ], response_model = Response , response_model_exclude_none = True , ) def predict ( user_id : int ) -> dict : \"\"\" Predict status from user ID features. Retrieve most recent features and make a prediction on the ``status``. cURL command: ```bash curl -X 'GET' 'http://localhost:5000/{user_id}/predict' -H 'accept: application/json' ``` Parameters: user_id (int): User ID. Returns: Response with ``prediction``. \"\"\" data = serve_features ( user_id )[ \"data\" ] if data [ \"found\" ]: data [ \"prediction\" ] = int ( model . predict ([ data [ \"features\" ]])) return { \"data\" : data } serve_features ( user_id ) Serve features from a user ID. Retrieve age , years_on_the_job , nb_previous_loans , avg_amount_loans_previous and flag_own_car most recent features. cURL command: curl -X 'GET' 'http://localhost:5000/{user_id}' -H 'accept: application/json' Parameters: user_id ( int ) \u2013 User ID. Returns: dict \u2013 Response with features . Source code in app/api.py @app . get ( \"/ {user_id} \" , tags = [ \"Features\" ], response_model = Response , response_model_exclude_none = True , ) def serve_features ( user_id : int ) -> dict : \"\"\" Serve features from a user ID. Retrieve ``age``, ``years_on_the_job``, ``nb_previous_loans``, ``avg_amount_loans_previous`` and ``flag_own_car`` most recent features. cURL command: ```bash curl -X 'GET' 'http://localhost:5000/{user_id}' -H 'accept: application/json' ``` Parameters: user_id (int): User ID. Returns: Response with ``features``. \"\"\" data = { \"user_id\" : user_id , \"found\" : user_id in df [ \"id\" ] . values , \"features\" : None , } if data [ \"found\" ]: ser = df [ df [ \"id\" ] == user_id ] . iloc [ - 1 ] ser . drop ([ \"id\" , \"status\" ], inplace = True ) data [ \"features\" ] = ser return { \"data\" : data }","title":"API Documentation"},{"location":"app/api/#api-documentation","text":"The MLOps API can be requested from the localhost on port 5000 .","title":"API Documentation"},{"location":"app/api/#app.api.predict","text":"Predict status from user ID features. Retrieve most recent features and make a prediction on the status . cURL command: curl -X 'GET' 'http://localhost:5000/{user_id}/predict' -H 'accept: application/json' Parameters: user_id ( int ) \u2013 User ID. Returns: dict \u2013 Response with prediction . Source code in app/api.py @app . get ( \"/ {user_id} /predict\" , tags = [ \"Prediction\" ], response_model = Response , response_model_exclude_none = True , ) def predict ( user_id : int ) -> dict : \"\"\" Predict status from user ID features. Retrieve most recent features and make a prediction on the ``status``. cURL command: ```bash curl -X 'GET' 'http://localhost:5000/{user_id}/predict' -H 'accept: application/json' ``` Parameters: user_id (int): User ID. Returns: Response with ``prediction``. \"\"\" data = serve_features ( user_id )[ \"data\" ] if data [ \"found\" ]: data [ \"prediction\" ] = int ( model . predict ([ data [ \"features\" ]])) return { \"data\" : data }","title":"predict()"},{"location":"app/api/#app.api.serve_features","text":"Serve features from a user ID. Retrieve age , years_on_the_job , nb_previous_loans , avg_amount_loans_previous and flag_own_car most recent features. cURL command: curl -X 'GET' 'http://localhost:5000/{user_id}' -H 'accept: application/json' Parameters: user_id ( int ) \u2013 User ID. Returns: dict \u2013 Response with features . Source code in app/api.py @app . get ( \"/ {user_id} \" , tags = [ \"Features\" ], response_model = Response , response_model_exclude_none = True , ) def serve_features ( user_id : int ) -> dict : \"\"\" Serve features from a user ID. Retrieve ``age``, ``years_on_the_job``, ``nb_previous_loans``, ``avg_amount_loans_previous`` and ``flag_own_car`` most recent features. cURL command: ```bash curl -X 'GET' 'http://localhost:5000/{user_id}' -H 'accept: application/json' ``` Parameters: user_id (int): User ID. Returns: Response with ``features``. \"\"\" data = { \"user_id\" : user_id , \"found\" : user_id in df [ \"id\" ] . values , \"features\" : None , } if data [ \"found\" ]: ser = df [ df [ \"id\" ] == user_id ] . iloc [ - 1 ] ser . drop ([ \"id\" , \"status\" ], inplace = True ) data [ \"features\" ] = ser return { \"data\" : data }","title":"serve_features()"},{"location":"config/config/","text":"Configuration Configure the directory names in config/config.py # Directories BASE_PATH = Path(__file__).parent.parent.resolve() CONFIG_DIR = Path(BASE_PATH, 'config') DATA_DIR = Path(BASE_PATH, 'data') TEST_DIR = Path(BASE_PATH, 'tests') MODEL_DIR = Path(BASE_PATH, 'models') Configure the model parameters in params.json { \"test_size\": 0.3, \"n_estimators\": 5, \"random_state\": 123 }","title":"Configuration"},{"location":"config/config/#configuration","text":"Configure the directory names in config/config.py # Directories BASE_PATH = Path(__file__).parent.parent.resolve() CONFIG_DIR = Path(BASE_PATH, 'config') DATA_DIR = Path(BASE_PATH, 'data') TEST_DIR = Path(BASE_PATH, 'tests') MODEL_DIR = Path(BASE_PATH, 'models') Configure the model parameters in params.json { \"test_size\": 0.3, \"n_estimators\": 5, \"random_state\": 123 }","title":"Configuration"},{"location":"mlops/data/","text":"mlops.data feature_engineering ( df ) Compute key features of the preprocessed dataset. Compute features of the df preprocessed dataset: age , years_on_the_job , nb_previous_loans , avg_amount_loans_previous and flag_own_car . Parameters: df ( DataFrame ) \u2013 Preprocessed dataset. Returns: DataFrame \u2013 Clean dataset. Source code in mlops/data.py def feature_engineering ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Compute key features of the preprocessed dataset. Compute features of the ``df`` preprocessed dataset: ``age``, ``years_on_the_job``, ``nb_previous_loans``, ``avg_amount_loans_previous`` and ``flag_own_car``. Parameters: df (DataFrame): Preprocessed dataset. Returns: Clean dataset. \"\"\" df = df . copy () df_grouped = df . groupby ( \"id\" ) df [ \"nb_previous_loans\" ] = df_grouped [ \"loan_date\" ] . rank ( method = \"first\" ) - 1 df [ \"avg_amount_loans_previous\" ] = df . groupby ( \"id\" )[ \"loan_amount\" ] . apply ( lambda x : x . shift () . expanding () . mean () ) df [ \"age\" ] = ( pd . to_datetime ( \"today\" ) . normalize () - df [ \"birthday\" ] ) . dt . days // 365 df [ \"years_on_the_job\" ] = ( pd . to_datetime ( \"today\" ) . normalize () - df [ \"job_start_date\" ] ) . dt . days // 365 df [ \"flag_own_car\" ] = df [ \"flag_own_car\" ] . apply ( lambda x : 0 if x == \"N\" else 1 ) return df [ [ \"id\" , \"age\" , \"years_on_the_job\" , \"nb_previous_loans\" , \"avg_amount_loans_previous\" , \"flag_own_car\" , \"status\" , ] ] preprocess ( df , inplace = False ) Preprocess the dataset. Sort dataset by id and loan_date . Convert all dates to datetime objects. Parameters: df ( DataFrame ) \u2013 Dataset. inplace ( bool ) \u2013 Modify the dataframe in place. Returns: DataFrame \u2013 Preprocessed dataset or None if inplace=True . Source code in mlops/data.py def preprocess ( df : pd . DataFrame , inplace : bool = False ) -> pd . DataFrame : \"\"\" Preprocess the dataset. Sort dataset by ``id`` and ``loan_date``. Convert all dates to ``datetime`` objects. Parameters: df (DataFrame): Dataset. inplace (bool, optional): Modify the dataframe in place. Returns: Preprocessed dataset or ``None`` if ``inplace=True``. \"\"\" if not inplace : df = df . copy () df . sort_values ( by = [ \"id\" , \"loan_date\" ], inplace = True ) df . reset_index ( drop = True , inplace = True ) df = _to_datetime ( df , columns = [ \"loan_date\" , \"birthday\" , \"job_start_date\" ], inplace = inplace , ) if not inplace : return df","title":"data"},{"location":"mlops/data/#mlopsdata","text":"","title":"mlops.data"},{"location":"mlops/data/#mlops.data.feature_engineering","text":"Compute key features of the preprocessed dataset. Compute features of the df preprocessed dataset: age , years_on_the_job , nb_previous_loans , avg_amount_loans_previous and flag_own_car . Parameters: df ( DataFrame ) \u2013 Preprocessed dataset. Returns: DataFrame \u2013 Clean dataset. Source code in mlops/data.py def feature_engineering ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Compute key features of the preprocessed dataset. Compute features of the ``df`` preprocessed dataset: ``age``, ``years_on_the_job``, ``nb_previous_loans``, ``avg_amount_loans_previous`` and ``flag_own_car``. Parameters: df (DataFrame): Preprocessed dataset. Returns: Clean dataset. \"\"\" df = df . copy () df_grouped = df . groupby ( \"id\" ) df [ \"nb_previous_loans\" ] = df_grouped [ \"loan_date\" ] . rank ( method = \"first\" ) - 1 df [ \"avg_amount_loans_previous\" ] = df . groupby ( \"id\" )[ \"loan_amount\" ] . apply ( lambda x : x . shift () . expanding () . mean () ) df [ \"age\" ] = ( pd . to_datetime ( \"today\" ) . normalize () - df [ \"birthday\" ] ) . dt . days // 365 df [ \"years_on_the_job\" ] = ( pd . to_datetime ( \"today\" ) . normalize () - df [ \"job_start_date\" ] ) . dt . days // 365 df [ \"flag_own_car\" ] = df [ \"flag_own_car\" ] . apply ( lambda x : 0 if x == \"N\" else 1 ) return df [ [ \"id\" , \"age\" , \"years_on_the_job\" , \"nb_previous_loans\" , \"avg_amount_loans_previous\" , \"flag_own_car\" , \"status\" , ] ]","title":"feature_engineering()"},{"location":"mlops/data/#mlops.data.preprocess","text":"Preprocess the dataset. Sort dataset by id and loan_date . Convert all dates to datetime objects. Parameters: df ( DataFrame ) \u2013 Dataset. inplace ( bool ) \u2013 Modify the dataframe in place. Returns: DataFrame \u2013 Preprocessed dataset or None if inplace=True . Source code in mlops/data.py def preprocess ( df : pd . DataFrame , inplace : bool = False ) -> pd . DataFrame : \"\"\" Preprocess the dataset. Sort dataset by ``id`` and ``loan_date``. Convert all dates to ``datetime`` objects. Parameters: df (DataFrame): Dataset. inplace (bool, optional): Modify the dataframe in place. Returns: Preprocessed dataset or ``None`` if ``inplace=True``. \"\"\" if not inplace : df = df . copy () df . sort_values ( by = [ \"id\" , \"loan_date\" ], inplace = True ) df . reset_index ( drop = True , inplace = True ) df = _to_datetime ( df , columns = [ \"loan_date\" , \"birthday\" , \"job_start_date\" ], inplace = inplace , ) if not inplace : return df","title":"preprocess()"},{"location":"mlops/main/","text":"mlops.main feature_engineering ( dataset_filename = 'dataset_credit_risk.csv' , clean_dataset_filename = 'api_dataset.pkl' ) Compute key features of the dataset. Load, preprocess and compute features of the dataset_filename dataset in the DATA_DIR directory. Finally, the clean dataset is saved as clean_dataset_filename in the same directory. Parameters: dataset_filename ( str ) \u2013 Dataset filename in the DATA_DIR directory. clean_dataset_filename ( str ) \u2013 Clean dataset filename in the DATA_DIR directory. Returns: DataFrame \u2013 Clean dataset. Source code in mlops/main.py @app . command () def feature_engineering ( dataset_filename : str = config . DATASET_FILENAME , clean_dataset_filename : str = config . CLEAN_DATASET_FILENAME , ) -> pd . DataFrame : \"\"\" Compute key features of the dataset. Load, preprocess and compute features of the ``dataset_filename`` dataset in the ``DATA_DIR`` directory. Finally, the clean dataset is saved as ``clean_dataset_filename`` in the same directory. Parameters: dataset_filename (str, optional): Dataset filename in the ``DATA_DIR`` directory. clean_dataset_filename (str, optional): Clean dataset filename in the ``DATA_DIR`` directory. Returns: Clean dataset. \"\"\" df = load_data ( dataset_filename ) df = data . preprocess ( df ) df = data . feature_engineering ( df ) if clean_dataset_filename is not None : clean_fp = Path ( config . DATA_DIR , clean_dataset_filename ) df . to_pickle ( clean_fp , protocol = 4 ) typer . echo ( \"Clean dataset file created at \" + str ( clean_fp . relative_to ( config . BASE_PATH )) ) return df load_data ( dataset_filename , nrows = None ) Load the dataset. Load dataset_filename dataset in the DATA_DIR directory. Parameters: dataset_filename ( str ) \u2013 Dataset filename in the DATA_DIR directory. nrows ( int ) \u2013 Number of rows to load, if None , load the entire dataset. Returns: DataFrame \u2013 Loaded dataset. Source code in mlops/main.py def load_data ( dataset_filename : str , nrows : int = None ) -> pd . DataFrame : \"\"\" Load the dataset. Load ``dataset_filename`` dataset in the ``DATA_DIR`` directory. Parameters: dataset_filename (str): Dataset filename in the ``DATA_DIR`` directory. nrows (int, optional): Number of rows to load, if ``None``, load the entire dataset. Returns: Loaded dataset. \"\"\" dataset_fp = Path ( config . DATA_DIR , dataset_filename ) return pd . read_csv ( dataset_fp , nrows = nrows ) predict ( x , model_filename = 'api.joblib' ) Predict the status value from a previous trained model. The model is loaded from the model_filename file in the MODEL_DIR directory to make a prediction based on the age , years_on_the_job , nb_previous_loans , avg_amount_loans_previous and flag_own_car features. Parameters: x ( list[float] ) \u2013 Feature vector. model_filename ( str ) \u2013 Filename of the model. Returns: int \u2013 Predicted status value. Source code in mlops/main.py @app . command () def predict ( x : list [ float ], model_filename : str = \"api.joblib\" ) -> int : \"\"\" Predict the ``status`` value from a previous trained model. The model is loaded from the ``model_filename`` file in the ``MODEL_DIR`` directory to make a prediction based on the ``age``, ``years_on_the_job``, ``nb_previous_loans``, ``avg_amount_loans_previous`` and ``flag_own_car`` features. Parameters: x (list[float]): Feature vector. model_filename (str, optional): Filename of the model. Returns: Predicted status value. \"\"\" model_fp = Path ( config . MODEL_DIR , model_filename ) model = load ( model_fp ) prediction = int ( model . predict ([ x ])) typer . echo ( \"Prediction: \" + str ( prediction )) return prediction train_model ( params_filename = 'params.json' , clean_filename = 'api_dataset.pkl' , model_filename = 'api.joblib' , metrics_filename = 'api_metrics.json' ) Train the model and compute the metrics. Load the model parameters from params_filename in the CONFIG_DIR directory and the clean dataset from clean_filename in the DATA_DIR directory. The model will be saved as model_filename and the metrics will be saved as metrics_filename in the MODEL_DIR directory. Parameters: params_filename ( str ) \u2013 Filename of the parameters. Parameters should be in a json file. clean_filename ( str ) \u2013 Filename of the clean dataset. model_filename ( str ) \u2013 Filename of the model. metrics_filename ( str ) \u2013 Filename of the metrics. Returns: dict \u2013 Artifacts of the trained model (parameters, model and metrics). Source code in mlops/main.py @app . command () def train_model ( params_filename : str = \"params.json\" , clean_filename : str = config . CLEAN_DATASET_FILENAME , model_filename : str = \"api.joblib\" , metrics_filename : str = \"api_metrics.json\" , ) -> dict [ str , Any ]: \"\"\" Train the model and compute the metrics. Load the model parameters from ``params_filename`` in the ``CONFIG_DIR`` directory and the clean dataset from ``clean_filename`` in the ``DATA_DIR`` directory. The model will be saved as ``model_filename`` and the metrics will be saved as ``metrics_filename`` in the ``MODEL_DIR`` directory. Parameters: params_filename (str, optional): Filename of the parameters. Parameters should be in a json file. clean_filename (str, optional): Filename of the clean dataset. model_filename (str, optional): Filename of the model. metrics_filename (str, optional): Filename of the metrics. Returns: Artifacts of the trained model (parameters, model and metrics). \"\"\" params_fp = Path ( config . CONFIG_DIR , params_filename ) params_dict = utils . load_dict ( params_fp ) params = namedtuple ( \"Params\" , params_dict . keys ())( * params_dict . values ()) clean_fp = Path ( config . DATA_DIR , clean_filename ) df = pd . read_pickle ( clean_fp ) artifacts = train . train ( df , params ) model_fp = Path ( config . MODEL_DIR , model_filename ) dump ( artifacts [ \"model\" ], model_fp ) metrics_fp = Path ( config . MODEL_DIR , metrics_filename ) utils . save_dict ( artifacts [ \"metrics\" ], metrics_fp ) typer . echo ( \"Model saved at \" + str ( model_fp . relative_to ( config . BASE_PATH ))) typer . echo ( \"Metrics saved at \" + str ( metrics_fp . relative_to ( config . BASE_PATH )) ) return artifacts","title":"main"},{"location":"mlops/main/#mlopsmain","text":"","title":"mlops.main"},{"location":"mlops/main/#mlops.main.feature_engineering","text":"Compute key features of the dataset. Load, preprocess and compute features of the dataset_filename dataset in the DATA_DIR directory. Finally, the clean dataset is saved as clean_dataset_filename in the same directory. Parameters: dataset_filename ( str ) \u2013 Dataset filename in the DATA_DIR directory. clean_dataset_filename ( str ) \u2013 Clean dataset filename in the DATA_DIR directory. Returns: DataFrame \u2013 Clean dataset. Source code in mlops/main.py @app . command () def feature_engineering ( dataset_filename : str = config . DATASET_FILENAME , clean_dataset_filename : str = config . CLEAN_DATASET_FILENAME , ) -> pd . DataFrame : \"\"\" Compute key features of the dataset. Load, preprocess and compute features of the ``dataset_filename`` dataset in the ``DATA_DIR`` directory. Finally, the clean dataset is saved as ``clean_dataset_filename`` in the same directory. Parameters: dataset_filename (str, optional): Dataset filename in the ``DATA_DIR`` directory. clean_dataset_filename (str, optional): Clean dataset filename in the ``DATA_DIR`` directory. Returns: Clean dataset. \"\"\" df = load_data ( dataset_filename ) df = data . preprocess ( df ) df = data . feature_engineering ( df ) if clean_dataset_filename is not None : clean_fp = Path ( config . DATA_DIR , clean_dataset_filename ) df . to_pickle ( clean_fp , protocol = 4 ) typer . echo ( \"Clean dataset file created at \" + str ( clean_fp . relative_to ( config . BASE_PATH )) ) return df","title":"feature_engineering()"},{"location":"mlops/main/#mlops.main.load_data","text":"Load the dataset. Load dataset_filename dataset in the DATA_DIR directory. Parameters: dataset_filename ( str ) \u2013 Dataset filename in the DATA_DIR directory. nrows ( int ) \u2013 Number of rows to load, if None , load the entire dataset. Returns: DataFrame \u2013 Loaded dataset. Source code in mlops/main.py def load_data ( dataset_filename : str , nrows : int = None ) -> pd . DataFrame : \"\"\" Load the dataset. Load ``dataset_filename`` dataset in the ``DATA_DIR`` directory. Parameters: dataset_filename (str): Dataset filename in the ``DATA_DIR`` directory. nrows (int, optional): Number of rows to load, if ``None``, load the entire dataset. Returns: Loaded dataset. \"\"\" dataset_fp = Path ( config . DATA_DIR , dataset_filename ) return pd . read_csv ( dataset_fp , nrows = nrows )","title":"load_data()"},{"location":"mlops/main/#mlops.main.predict","text":"Predict the status value from a previous trained model. The model is loaded from the model_filename file in the MODEL_DIR directory to make a prediction based on the age , years_on_the_job , nb_previous_loans , avg_amount_loans_previous and flag_own_car features. Parameters: x ( list[float] ) \u2013 Feature vector. model_filename ( str ) \u2013 Filename of the model. Returns: int \u2013 Predicted status value. Source code in mlops/main.py @app . command () def predict ( x : list [ float ], model_filename : str = \"api.joblib\" ) -> int : \"\"\" Predict the ``status`` value from a previous trained model. The model is loaded from the ``model_filename`` file in the ``MODEL_DIR`` directory to make a prediction based on the ``age``, ``years_on_the_job``, ``nb_previous_loans``, ``avg_amount_loans_previous`` and ``flag_own_car`` features. Parameters: x (list[float]): Feature vector. model_filename (str, optional): Filename of the model. Returns: Predicted status value. \"\"\" model_fp = Path ( config . MODEL_DIR , model_filename ) model = load ( model_fp ) prediction = int ( model . predict ([ x ])) typer . echo ( \"Prediction: \" + str ( prediction )) return prediction","title":"predict()"},{"location":"mlops/main/#mlops.main.train_model","text":"Train the model and compute the metrics. Load the model parameters from params_filename in the CONFIG_DIR directory and the clean dataset from clean_filename in the DATA_DIR directory. The model will be saved as model_filename and the metrics will be saved as metrics_filename in the MODEL_DIR directory. Parameters: params_filename ( str ) \u2013 Filename of the parameters. Parameters should be in a json file. clean_filename ( str ) \u2013 Filename of the clean dataset. model_filename ( str ) \u2013 Filename of the model. metrics_filename ( str ) \u2013 Filename of the metrics. Returns: dict \u2013 Artifacts of the trained model (parameters, model and metrics). Source code in mlops/main.py @app . command () def train_model ( params_filename : str = \"params.json\" , clean_filename : str = config . CLEAN_DATASET_FILENAME , model_filename : str = \"api.joblib\" , metrics_filename : str = \"api_metrics.json\" , ) -> dict [ str , Any ]: \"\"\" Train the model and compute the metrics. Load the model parameters from ``params_filename`` in the ``CONFIG_DIR`` directory and the clean dataset from ``clean_filename`` in the ``DATA_DIR`` directory. The model will be saved as ``model_filename`` and the metrics will be saved as ``metrics_filename`` in the ``MODEL_DIR`` directory. Parameters: params_filename (str, optional): Filename of the parameters. Parameters should be in a json file. clean_filename (str, optional): Filename of the clean dataset. model_filename (str, optional): Filename of the model. metrics_filename (str, optional): Filename of the metrics. Returns: Artifacts of the trained model (parameters, model and metrics). \"\"\" params_fp = Path ( config . CONFIG_DIR , params_filename ) params_dict = utils . load_dict ( params_fp ) params = namedtuple ( \"Params\" , params_dict . keys ())( * params_dict . values ()) clean_fp = Path ( config . DATA_DIR , clean_filename ) df = pd . read_pickle ( clean_fp ) artifacts = train . train ( df , params ) model_fp = Path ( config . MODEL_DIR , model_filename ) dump ( artifacts [ \"model\" ], model_fp ) metrics_fp = Path ( config . MODEL_DIR , metrics_filename ) utils . save_dict ( artifacts [ \"metrics\" ], metrics_fp ) typer . echo ( \"Model saved at \" + str ( model_fp . relative_to ( config . BASE_PATH ))) typer . echo ( \"Metrics saved at \" + str ( metrics_fp . relative_to ( config . BASE_PATH )) ) return artifacts","title":"train_model()"},{"location":"mlops/train/","text":"mlops.train split ( df , params ) Split the dataset into train and test sets. Set status column as the target value and set the age , years_on_the_job , nb_previous_loans , avg_amount_loans_previous and flag_own_car columns as the features. Use the SMOTE method to over-sample the train set. Parameters: df ( DataFrame ) \u2013 Clean dataset. params ( namedtuple ) \u2013 Parameters for test_size and random_state . Returns: tuple \u2013 X_train , X_test , y_train and y_test sets. Source code in mlops/train.py def split ( df : pd . DataFrame , params : namedtuple ) -> tuple [ pd . DataFrame , pd . DataFrame , pd . Series , pd . Series ]: \"\"\" Split the dataset into train and test sets. Set ``status`` column as the target value and set the ``age``, ``years_on_the_job``, ``nb_previous_loans``, ``avg_amount_loans_previous`` and ``flag_own_car`` columns as the features. Use the ``SMOTE`` method to over-sample the train set. Parameters: df (DataFrame): Clean dataset. params (namedtuple): Parameters for ``test_size`` and ``random_state``. Returns: ``X_train``, ``X_test``, ``y_train`` and ``y_test`` sets. \"\"\" df = df . copy () Y = df [ \"status\" ] . astype ( \"int\" ) df . drop ([ \"status\" ], axis = 1 , inplace = True ) df . drop ([ \"id\" ], axis = 1 , inplace = True ) X = df X_train , X_test , y_train , y_test = train_test_split ( X , Y , stratify = Y , test_size = params . test_size , random_state = params . random_state , ) sm = SMOTE ( random_state = params . random_state ) X_train , y_train = sm . fit_resample ( X_train , y_train ) return X_train , X_test , y_train , y_test train ( df , params ) Train a RandomForestClassifier on the dataset. Fill all NaN values with zero, split the data into train and test sets, fit the model and test to compute the metrics. Parameters: df ( DataFrame ) \u2013 Clean dataset. params ( namedtuple ) \u2013 Parameters for n_estimators and random_state . Returns: dict \u2013 Artifacts of the trained model (parameters, model and metrics). Source code in mlops/train.py def train ( df : pd . DataFrame , params : namedtuple ) -> dict [ str , Any ]: \"\"\" Train a ``RandomForestClassifier`` on the dataset. Fill all ``NaN`` values with zero, split the data into train and test sets, fit the model and test to compute the metrics. Parameters: df (DataFrame): Clean dataset. params (namedtuple): Parameters for ``n_estimators`` and ``random_state``. Returns: Artifacts of the trained model (parameters, model and metrics). \"\"\" df = df . copy () df . fillna ( 0 , inplace = True ) X_train , X_test , y_train , y_test = split ( df , params ) model = RandomForestClassifier ( n_estimators = params . n_estimators , random_state = params . random_state ) model . fit ( X_train . values , y_train . values ) y_predict = model . predict ( X_test . values ) artifacts = { \"params\" : params , \"model\" : model , \"metrics\" : { \"accuracy\" : accuracy_score ( y_test , y_predict ), \"precision\" : precision_score ( y_test , y_predict ), \"recall\" : recall_score ( y_test , y_predict ), \"conf_matrix\" : confusion_matrix ( y_test , y_predict ) . tolist (), }, } return artifacts","title":"train"},{"location":"mlops/train/#mlopstrain","text":"","title":"mlops.train"},{"location":"mlops/train/#mlops.train.split","text":"Split the dataset into train and test sets. Set status column as the target value and set the age , years_on_the_job , nb_previous_loans , avg_amount_loans_previous and flag_own_car columns as the features. Use the SMOTE method to over-sample the train set. Parameters: df ( DataFrame ) \u2013 Clean dataset. params ( namedtuple ) \u2013 Parameters for test_size and random_state . Returns: tuple \u2013 X_train , X_test , y_train and y_test sets. Source code in mlops/train.py def split ( df : pd . DataFrame , params : namedtuple ) -> tuple [ pd . DataFrame , pd . DataFrame , pd . Series , pd . Series ]: \"\"\" Split the dataset into train and test sets. Set ``status`` column as the target value and set the ``age``, ``years_on_the_job``, ``nb_previous_loans``, ``avg_amount_loans_previous`` and ``flag_own_car`` columns as the features. Use the ``SMOTE`` method to over-sample the train set. Parameters: df (DataFrame): Clean dataset. params (namedtuple): Parameters for ``test_size`` and ``random_state``. Returns: ``X_train``, ``X_test``, ``y_train`` and ``y_test`` sets. \"\"\" df = df . copy () Y = df [ \"status\" ] . astype ( \"int\" ) df . drop ([ \"status\" ], axis = 1 , inplace = True ) df . drop ([ \"id\" ], axis = 1 , inplace = True ) X = df X_train , X_test , y_train , y_test = train_test_split ( X , Y , stratify = Y , test_size = params . test_size , random_state = params . random_state , ) sm = SMOTE ( random_state = params . random_state ) X_train , y_train = sm . fit_resample ( X_train , y_train ) return X_train , X_test , y_train , y_test","title":"split()"},{"location":"mlops/train/#mlops.train.train","text":"Train a RandomForestClassifier on the dataset. Fill all NaN values with zero, split the data into train and test sets, fit the model and test to compute the metrics. Parameters: df ( DataFrame ) \u2013 Clean dataset. params ( namedtuple ) \u2013 Parameters for n_estimators and random_state . Returns: dict \u2013 Artifacts of the trained model (parameters, model and metrics). Source code in mlops/train.py def train ( df : pd . DataFrame , params : namedtuple ) -> dict [ str , Any ]: \"\"\" Train a ``RandomForestClassifier`` on the dataset. Fill all ``NaN`` values with zero, split the data into train and test sets, fit the model and test to compute the metrics. Parameters: df (DataFrame): Clean dataset. params (namedtuple): Parameters for ``n_estimators`` and ``random_state``. Returns: Artifacts of the trained model (parameters, model and metrics). \"\"\" df = df . copy () df . fillna ( 0 , inplace = True ) X_train , X_test , y_train , y_test = split ( df , params ) model = RandomForestClassifier ( n_estimators = params . n_estimators , random_state = params . random_state ) model . fit ( X_train . values , y_train . values ) y_predict = model . predict ( X_test . values ) artifacts = { \"params\" : params , \"model\" : model , \"metrics\" : { \"accuracy\" : accuracy_score ( y_test , y_predict ), \"precision\" : precision_score ( y_test , y_predict ), \"recall\" : recall_score ( y_test , y_predict ), \"conf_matrix\" : confusion_matrix ( y_test , y_predict ) . tolist (), }, } return artifacts","title":"train()"},{"location":"mlops/utils/","text":"mlops.utils load_dict ( filepath ) Load json file as dictionary. Open a json file and convert to a dictionary. Parameters: filepath ( str ) \u2013 Path to load the json file. Returns: dict \u2013 Dictionary. Source code in mlops/utils.py def load_dict ( filepath : str ) -> dict : \"\"\" Load json file as dictionary. Open a json file and convert to a dictionary. Parameters: filepath (str): Path to load the json file. Returns: Dictionary. \"\"\" with open ( filepath , \"r\" ) as fp : d = json . load ( fp ) return d save_dict ( d , filepath ) Save dictionary as a json file. Convert dictionary to a json file and save in filepath . Parameters: d ( dict ) \u2013 Dictionary. filepath ( str ) \u2013 Path to save the json file. Source code in mlops/utils.py def save_dict ( d : dict , filepath : str ) -> None : \"\"\" Save dictionary as a json file. Convert dictionary to a json file and save in ``filepath``. Parameters: d (dict): Dictionary. filepath (str): Path to save the json file. \"\"\" with open ( filepath , \"w\" ) as fp : json . dump ( d , fp = fp , indent = 2 )","title":"utils"},{"location":"mlops/utils/#mlopsutils","text":"","title":"mlops.utils"},{"location":"mlops/utils/#mlops.utils.load_dict","text":"Load json file as dictionary. Open a json file and convert to a dictionary. Parameters: filepath ( str ) \u2013 Path to load the json file. Returns: dict \u2013 Dictionary. Source code in mlops/utils.py def load_dict ( filepath : str ) -> dict : \"\"\" Load json file as dictionary. Open a json file and convert to a dictionary. Parameters: filepath (str): Path to load the json file. Returns: Dictionary. \"\"\" with open ( filepath , \"r\" ) as fp : d = json . load ( fp ) return d","title":"load_dict()"},{"location":"mlops/utils/#mlops.utils.save_dict","text":"Save dictionary as a json file. Convert dictionary to a json file and save in filepath . Parameters: d ( dict ) \u2013 Dictionary. filepath ( str ) \u2013 Path to save the json file. Source code in mlops/utils.py def save_dict ( d : dict , filepath : str ) -> None : \"\"\" Save dictionary as a json file. Convert dictionary to a json file and save in ``filepath``. Parameters: d (dict): Dictionary. filepath (str): Path to save the json file. \"\"\" with open ( filepath , \"w\" ) as fp : json . dump ( d , fp = fp , indent = 2 )","title":"save_dict()"}]}